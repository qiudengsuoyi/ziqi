#浅谈搜索引擎中的Robots协议#
在网上使用搜索引擎搜索Robots协议，总有360与百度大战后留下的对于Robots协议的探讨。对于百度Robots协议禁止360引擎蜘蛛对其下各大页面进行抓取，以及奇虎360通过技术手段绕过协议规则进行强取的做法各方人士都有争执，分别也从反不正当竞争以及市场垄断的角度进行了分析，也都颇有道理。我在这里不去划分孰对孰错，而只是粗陋地谈谈Robots协议的意义与价值。

这里我引用[协议的原则][1]：
>*  搜索技术应服务于人类，同时尊重信息提供者的意愿，并维护其隐私权；
>*  网站有义务保护其使用者的个人信息和隐私不被侵犯。

互联网是个大世界，技术迅速衍生与发展是同明文条例约束缺失的现实情况同存的。同时，很多技术运用在实际环境时，是难以定义其行为是否规范的，就像雾里看花，总是隔着一层朦胧。而在此情况下，各行各业总会通过不断地实践衍生出各个技术领域的行业规范或者道德规范。而形成的这个结果是具有意义的。就如robots协议提出那样，从1994年开始，robots协议就被Google、Yahoo、微软、百度等各大正规搜索引擎当作行业标准，遵循协议所制定规则，让自己的蜘蛛在遵照信息提供方或者传递方意愿的前提下满足自身需求。这是双方在兼顾双方利益的同时，尽可能地实现双赢的协议。减缓了信息拥有方的服务器负荷，也减少了爬取方的无用信息，提升效率。

不要在协议上小题大做，信息拥有方应当本着公平的精神去限定爬取的蜘蛛。就如百度而言，对待对手奇虎360时，除了在考虑竞争关系的基础上，若能做到不一概而论，便能展现出其为国内搜索引擎大哥的风范。自己所构建的庞大的信息帝国，一砖一瓦都有小信息拥有者的功劳，喝水思源，才能使得其在协议书写上更为合理，更好的实现双赢。

不要刻意逾越道德的围墙，信息爬取方应当以协议为前提，尊重信息提供者的隐私与意愿。通过技术逾越规范会造成双方矛盾的，相互攻击也会成为最终结果，即时的牟利未必会得利。

总的来说，Robots协议提出以及使用是有意义的，它划定了双方的界限，保证整个行业生态，使得搜索这棵大树强壮年轻。
[1]: http://baike.baidu.com/link?url=q2GLxJJtjU7EOuatkNaFOBXF44SctfhcFJU1M7WlEm5g1LyYL_y5CqYXp8CrhOWmWbjyD7kHQFzvIJtGX1MJwB4az-ICvWEdDED0yeje5KgSwjfQJfqPtUn5euayVsb8